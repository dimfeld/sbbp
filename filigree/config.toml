product_name = "SBBP"
company_name = "Daniel Imfeld"

default_auth_scope = "model"

api_dir = "api"
web_dir = "web"


shared_types = [
  "crate::models::video::VideoProcessingState",
]

[secrets]
deepgram = "DEEPGRAM_API_KEY"
anthropic = "ANTHROPIC_API_KEY"

[server]
dotenv = true
hosts = ["localhost"]

[formatter]
js = ["biome", "format", "--stdin-file-path=stdin.ts" ]
rust = ["rustfmt", "+nightly"]
sql = ["pg_format"]

[database]
migrate_on_start = true

[email]
provider = "none"
from = "daniel@imfeld.dev"

[storage.bucket.uploads]
bucket = "sbbp-uploads"
provider = "disk"

[storage.bucket.images]
bucket = "sbbp-images"
provider = "disk"

[storage.provider.disk]
type = "local"
base_path = "storage"


# Job config
# Use a separate task for each stage so that we can pipeline downloads and processing.
# Also makes it easier to rerun specific stages later on.

[job.download]
worker = "download"

[job.extract]
worker = "compute"

[job.analyze]
worker = "compute"

[job.transcribe]
worker = "transcribe"

[job.summarize]
worker = "summarize"

[worker.download]
max_concurrency = 2

[worker.compute]
max_concurrency = 8

[worker.transcribe]
max_concurrency = 16

[worker.summarize]
max_concurrency = 16

